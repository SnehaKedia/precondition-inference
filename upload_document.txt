Complete this document and upload along with your prediction results and your code.

### Method Name ###
Use a phrasal name to describe your method, e.g. training a BiLSTM cross-encoder from scratch, fine-tuning RoBERTa-large-MNLI, etc.

----> Fine-tuning ALBERT-large-v2

### Sentence pair encoder ###
Use up to 5 sentences to describe your encoder for the sentence pairs. Need to mention the following:
- Is it a bi-encoder or cross-encoder?
- What type of encoder (LSTM, Transformer, etc.)
- Is it based on a pre-trained model (BERT-large? RoBERTa-large-SNLI? BART-large-MNLI?) or completely trained from scratch by yourself (then how do you characterize the words and aggregate them into sentence representations)?

----> Used a pre-trained ALBERT model from huggingface for the precondition inference task. It is a cross-encoder, transformer model. 
The ALBERT Large v2 is a transformers model pre-trained on a large corpus of English data in a self-supervised fashion using a masked language modeling (MLM) objective.
This model has the following configuration: 24 repeating layers, 128 embedding dimension, 1024 hidden dimension, 16 attention heads, 17M parameters.


### Training & Development ###
Up to 5 sentences: how did you evaluate your solution using the dev set before submitting to the leaderboard? What are some key hyperparameter values (e.g., optimizer, learning rate, batch size, etc.)? How did you terminate the training (using a fixed #epochs, early stopping based on dev set performance)?

----> While training the model, I kept saving the model every time there was an improvement in the loss from the previously saved model. I had a fixed number of epochs. Also, I used accuracy and f1 score for evaluating the model performance on the dev set. Optimizer used was Adam Optimizer from PyTorch, the learning rate was 2e-5 and batch size was 32.

### Other methods ###
Did you try other methods than the submitted one?

----> I tried using the following methods apart from the one submitted:
* LSTM
* Enhanced Sequential Inference Model or ESIM
* Enhanced Sequential Inference Model or ESIM
* BERT Base V2 from hugging face
* Roberta Base from hugging face
* Albert Base v2 from hugging face

### Packages ###
List the key python packages you have used in this assignment.

----> transformers, torch, datasets, numpy, pandas, os, matplotlib, tqdm